
<!doctype html>
<html lang="en">
  <head>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Other -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css">

    <title>Jonah Philion</title>
  </head>
  <body>

    <!-- header -->
    <!-- <div class='jumbotron' style="background-color:#e6e9ec">
    <div class="container">
    <h1 class="text-center">Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</h1>
    <p class='text-center'><a href="https://scholar.google.com/citations?user=VVIAoY0AAAAJ&hl=en" target="_blank">Jonah Philion</a>, <a href="http://www.cs.toronto.edu/~fidler/" target="_blank">Sanja Fidler</a></p>
    <p class='text-center'>NVIDIA, Vector Institute, University of Toronto</p>
    <p class='text-center'>ECCV 2020</p>
    <p class='text-center'><img src='imgs/nusc.gif' class='img-fluid' style='height:250px; border-radius:15px; padding:5px'></p>
    <!-- <iframe src="https://drive.google.com/file/d/1XwqzDYfzXhky1WNuXTKy7i-jVXgp4hc_/preview" width="640" height="480" autoplay></iframe> -->
    <!-- <div class="embed-responsive embed-responsive-16by9" style='height:250px; width:444px; margin:auto;'>
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/oL5ISk6BnDE?autoplay=1" allowfullscreen></iframe>
    </div> -->
    <!-- </div>
    </div> -->

    <div class="container">

      <!-- <p>
        The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eye- view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’s- eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar.
      </p>

    <hr/> -->

    <!-- <span class="border border-white">
    <h4 class="text-center">News</h4>
    <ul>
      <li>[October 2020] code release on <a href='https://github.com/nv-tlabs/lift-splat-shoot' target="_blank">github</a></li>
      <li>[August 2020] paper released on <a href='https://arxiv.org/abs/2008.05711' target="_blank">arxiv</a></li>
    </ul>
    </span>

    <hr/> -->

    <span class="border border-white">
      <!-- <h4 class="text-center">Paper</h4> -->
      <div class='row'>
        <div class='col-sm-4'>
          <img src='Philion.jpg' class='img-fluid float-right' style='height:220px; border: solid; border-width: thin; border-radius:30px; border-color:#000000;'>
        </div>
        <div class='col'>
          <h5 class="card-title text-center">Jonah Philion</h5>
          <p class="card-text">I'm a PhD student at University of Toronto working on machine learning and computer vision, primarily with applications to self-driving. I'm advised by <a href=https://www.cs.utoronto.ca/~fidler/>Sanja Fidler</a>. I'm also a part-time research scientist at <a href=https://www.nvidia.com/en-us/research/>NVIDIA</a>.</p>
          <p class='card-text'>
              Before coming to University of Toronto, I worked for a year on the perception team at <a href='https://www.isee.ai/'>ISEE</a>. I did my undergrad at Harvard University where I studied Physics and Math and minored in computer science. During college, I was also part of the <a href=https://college.harvard.edu/academics/liberal-arts-sciences/dual-degree-music-programs>Harvard/NEC program</a> for jazz performance (alto saxophone).
          </p>
          <p class='text-center'>
            <a href=https://twitter.com/PhilionJonah>Twitter</a> / <a href=https://github.com/jonahthelion>Github</a> / <a href='./jonah_philion_cv.pdf'>CV</a>
          </p>
        </div>
      </div>
    </span>

    <hr/>
    <h4 class='text-center'>Research</h4>


    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='./figs/invrend.jpg' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text">Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting<br>
            Zian Wang, Jonah Philion, Sanja Fidler, Jan Kautz
        <br>ICCV 2021 (<span class="text-success">oral</span>)</p>

        <p>Coming soon!</p>
        </div>
    </div>
    </span>


    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='figs/drivegan.png' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text"><a href=https://nv-tlabs.github.io/DriveGAN/>DriveGAN: Towards a Controllable High-Quality Neural Simulation</a><br>
            Seung Wook Kim, Jonah Philion, Antonio Torralba, Sanja Fidler
        <br>CVPR 2021 (<span class="text-success">oral</span>)</p>

        <p><a href=https://nv-tlabs.github.io/DriveGAN/>project page</a> / <a href=https://arxiv.org/abs/2104.15060>arXiv</a> / <a href=https://nv-tlabs.github.io/DriveGAN/assets/bibtex.bib>bibtex</a></p>
        </div>
    </div>
    </span>


    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='figs/emergent.png' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text"><a href=https://fidler-lab.github.io/social-driving/>Emergent Road Rules In Multi-Agent Driving Environments</a><br>
            Avik Pal, Jonah Philion, Yuan-Hong Liao, Sanja Fidler
        <br>ICLR 2020</p>

        <p><a href=https://fidler-lab.github.io/social-driving/>project page</a> / <a href=https://arxiv.org/abs/2011.10753>arXiv</a> / <a href=https://github.com/fidler-lab/social-driving>code</a> / <a href=https://fidler-lab.github.io/social-driving/bibtex.bib>bibtex</a></p>
        </div>
    </div>
    </span>


    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='figs/nuscpkl.png' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text"><a href=https://arxiv.org/abs/2010.09350>The Efficacy of Neural Planning Metrics: A Meta-Analysis of PKL on nuScenes</a><br>
            Yiluan Guo, Holger Caesar, Oscar Beijbom, Jonah Philion, Sanja Fidler
        <br>IROS 2020 Workshop on Benchmarking Progress in Autonomous Driving</p>

        <p><a href=https://www.nuscenes.org/object-detection>nuScenes leaderboard</a> / <a href=https://arxiv.org/abs/2010.09350>arXiv</a> / <a href=https://github.com/nv-tlabs/planning-centric-metrics>code</a></p>
        </div>
    </div>
    </span>

    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='figs/lss.jpg' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text"><a href=https://nv-tlabs.github.io/lift-splat-shoot/>Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</a><br>
            Jonah Philion, Sanja Fidler
        <br>ECCV 2020</p>

        <p><a href=https://nv-tlabs.github.io/lift-splat-shoot/>project page</a> / <a href=https://arxiv.org/abs/2008.05711>arXiv</a> / <a href=https://github.com/nv-tlabs/lift-splat-shoot>code</a> / <a href=https://youtu.be/oL5ISk6BnDE>video</a> / <a href=https://nv-tlabs.github.io/lift-splat-shoot/cite.txt>bibtex</a></p>
        </div>
    </div>
    </span>


    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='figs/gamegan.png' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text"><a href=https://nv-tlabs.github.io/gameGAN/>Learning to Simulate Dynamic Environments With GameGAN</a><br>
            Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler
        <br>ECCV 2020</p>

        <p><a href=https://nv-tlabs.github.io/gameGAN/>project page</a> / <a href=https://arxiv.org/abs/2005.12126>arXiv</a> / <a href=https://github.com/nv-tlabs/GameGAN_code>code</a> / <a href=https://www.youtube.com/watch?v=4OzJUNsPx60&ab_channel=NVIDIA>video</a> / <a href=https://nv-tlabs.github.io/gameGAN/assets/bibtex.bib>bibtex</a></p>
        </div>
    </div>
    </span>

    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='figs/pkl.png' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text"><a href=https://nv-tlabs.github.io/detection-relevance/>Learning to Evaluate Perception Models Using Planner-Centric Metrics</a><br>
            Jonah Philion, Amlan Kar, Sanja Fidler
        <br>ECCV 2020</p>

        <p><a href=https://nv-tlabs.github.io/detection-relevance/>project page</a> / <a href=https://arxiv.org/abs/2004.08745>arXiv</a> / <a href=https://github.com/nv-tlabs/planning-centric-metrics>code</a> / <a href=https://nv-tlabs.github.io/detection-relevance/cite.txt>bibtex</a></p>
        </div>
    </div>
    </span>


    <span class="border border-white">
    <div class='row'>
        <div class='col-sm-4'>
        <img src='figs/fastdraw.png' class='img-fluid float-right' style='height:150px; border: solid; border-width: thin; border-radius:15px; border-color:#000000;'>
        </div>
        <div class='col-sm-6'>
        <p class="card-text"><a href=https://arxiv.org/abs/1905.04354>FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network</a><br>
            Jonah Philion
        <br>CVPR 2019</p>

        <p><a href=https://arxiv.org/abs/1905.04354>arXiv</a> / <a href=https://github.com/jonahthelion/FastDraw>code</a></p>
        </div>
    </div>
    </span>

    <!-- <hr/>
    <h4 class='text-center'>Music</h4> -->




    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>